---
sidebar_position: 3
---

# ViLBERT：序幕中的交織

**[ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks (2019.08)](https://arxiv.org/abs/1908.02265)**

---

:::info
以下內容由 ChatGPT-4 彙整，並經過人工校對編輯與補充說明。
:::

---

本篇文章要介紹的 ViLBERT 和上一篇的 VisualBERT 是同一個時期的作品。

學術界的每一年都會發生很多事情，而 2019 年特別多，事情遠遠還沒到結束的時候。

我們都知道，要讓機器真正「看懂」一張圖片，並用語言描述出來，這絕非易事。過去，大多數的研究方法是將圖片理解和語言處理分開來進行。但這樣的做法，當我們嘗試將這兩者結合時，結果往往不盡人意。就好像機器能辨識出圖片中是一隻柴犬，但當你問它「這是什麼品種的狗？」，它可能束手無策。

於是，本論文提出了一個新的方法：ViLBERT。不同於傳統的學習方式，ViLBERT 希望從一開始就讓機器同時學習圖片和語言。例如：當給機器一張紅色的蘋果照片和文字「這是紅色的蘋果」，ViLBERT 會努力讓機器明白「紅色」這個描述與蘋果的顏色之間的關聯。

根據初步的研究，ViLBERT 在多項測試中都展現出色的表現，遠超過其他方法。特別是在問答遊戲中，ViLBERT 不僅能「看懂」圖片，還能給出更精準的回答。

接著，我們來看一下在本論文中，作者如何定義問題，並且提供了什麼解決方式。

## 定義問題

1. 視覺與語言的結合：雖然這兩個領域在各自的範疇中都取得了不少進展，但要結合起來仍然具有挑戰性。目前的方法多半是「分開訓練視覺和語言模型」，然後再試圖結合它們。這樣的方式往往導致不理想的結果，因為當視覺和語言資料有限或有偏差時，模型的泛化能力較差。

2. 有效的視覺基礎：儘管電腦可以辨識圖片中的物體或理解語言，但如何讓這兩者關聯起來仍是一大問題。例如：電腦可能可以辨識出圖片中的一隻狗，但可能無法與「柴犬」或「牧羊犬」這樣的語言概念聯繫起來。